# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.16.1
#   kernelspec:
#     display_name: dswx_val
#     language: python
#     name: dswx_val
# ---

# %% [markdown]
# # Introduction
# **Warning**: It is not recommended to run (let alone modify) this notebook - unless you really know what you are doing. This categorizes the metadata for validation and is kept only for provenance of how databases are archived and organized.
#
# **Warning**: Since the DSWx-HLS database has been publicly posted, the s3 urls and ES database may not be available. See the [README.md](../Readme.md) for download details. The dataset can be found on earthdata [here](https://search.earthdata.nasa.gov/search/granules?p=C2603501575-POCLOUD&pg[0][v]=f&pg[0][gsk]=-start_date&q=dswx&tl=1701297419!3!!).
#
# This notebook is designed to do necessary accounting and organization of the validation datasets and provisional products so that they can be looked up for the remainder of the requirement verification.  Firstly, we regenerate the Validation Table that links the following datasets:
#
# 1. Classified Planet Imagery (and their urls)
# 2. DSWx Products (and their urls)
# 3. HLS Products
#
# This notebook puts the said table in the `dswx_verification` package so we can read this table using this module as a package. Then, we localize the data and include the relative paths in the above table. The localized data is also zipped so that it can be sent to the DAAC; this notebook serves to trace the creation of the validation data.
#
# We remark that we also link the so-called "site_name", which is an ID carried over from [Pickens, et al.'s work](https://www.sciencedirect.com/science/article/pii/S0034425720301620). The sites in total are chosen to provide a global stratififed sample of inland water. 
#
# **Note**: In order to obtain the latest datasets generated by the provisional system, this notebook requires JPL VPN access and a `.env` file created as described in the readme of this repository! Although all the datasets are located in public buckets, the database that makes the searching of these datasets possible is not.

# %%
import datetime
import rasterio
from pathlib import Path
import json
from tqdm import tqdm
from itertools import starmap
import geopandas as gpd
import requests
import concurrent.futures
import shutil

import dswx_verification
from dswx_verification import generate_linked_id_table_for_classified_imagery, get_path_of_validation_geojson
from dswx_verification.val_db import get_localized_validation_table, get_classified_planet_table

# %% [markdown]
# # Generate a new Validation Table using the Elastic Storage (ES) Database
#
# We can skip this step entirely by setting the below variable to `False`. This will mean that only the localization step is used and we will use the latest table to obtain valid s3 links. This is important because sometimes HySDS Elastic Storage (ES) database will be offline even though the S3 links are still valid.

# %%
REGENERATE_TABLE_FOR_DSWX_HLS_WITH_ES = False

# %%
if not REGENERATE_TABLE_FOR_DSWX_HLS_WITH_ES:
    df = get_localized_validation_table()
else:
    df = generate_linked_id_table_for_classified_imagery()
print(df.shape)
df.head()

# %% [markdown]
# # Localize Data
#
# This step both:
#
# 1. Localizes the data
# 2. Includes the relative paths of the data into the table.
#
# The first part of this step can be ignored (say if you just want to update the table) by setting the variable in the following cell to `False`.

# %%
LOCALIZE_DATA = False

# %%
t = datetime.date.today()
t

# %%
local_db_dir = Path(f'opera_dswx_val_db-{t.year}{t.month:02d}{t.day:02d}')
local_db_dir.mkdir(exist_ok=True, parents=True)
local_db_dir

# %%
df_planet = get_classified_planet_table()
df_planet.head()


# %% [markdown]
# A few important notes in what follows below. First, we need to download each DSWx file using requests in its entirity so we get all the metadata (tags, colorbar, etc). Second, there are 10 separate files for each DSWx dataset and the DEM is by far the largest. For performance, it's best to fashion a queue for all the files at once (10 x number of datasets) so that we can allow the smaller files to continue to occupy available workers while DEMs are downloaded. The validation datasets are each 1 file and are small. We download these datasets serially.

# %%
def download_one(url: str,
                 out_dir: Path,
                 out_file_name: str = None,
                 localize_data=LOCALIZE_DATA) -> Path:
    

    local_file_name = out_file_name or url.split('/')[-1]
    out_path = out_dir / local_file_name

    if localize_data:
        resp = requests.get(url, stream=True)
        with open(out_path, 'wb') as file:
            for chunk in resp.iter_content(chunk_size=1024):  
                file.write(chunk)
    return out_path

def localize_dswx_data(df: gpd.GeoDataFrame,
                       localize_data=LOCALIZE_DATA,
                       max_workers=20) -> list[str]:
    dswx_urls_by_site = df.dswx_hls_urls.tolist()
    dswx_urls_all = [url for url_group in dswx_urls_by_site for url in url_group.split(' ')]
    site_names = df.site_name.tolist()
    out_dirs = [local_db_dir / site_name / 'dswx'  for (site_name, url_group) in zip(site_names, dswx_urls_by_site) 
                                                   for url in url_group.split(' ')]
    if localize_data:
        [out_dir.mkdir(exist_ok=True, parents=True) for out_dir in out_dirs]

    def download_one_p(data):
        url, out_dir = data
        return download_one(url, out_dir) 
    
    input_data = list(zip(dswx_urls_all, out_dirs))
    n = len(input_data)
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        out_paths = list(tqdm(executor.map(download_one_p, input_data), total=n, desc='dwsx_files'))
    return out_paths

def localize_val_data(df: gpd.GeoDataFrame,
                      max_workers=10,
                      localize_data=LOCALIZE_DATA) -> list[str]:
    val_urls = df.validation_dataset_url
    site_names = df.site_name
    planet_ids = df.planet_id

    out_dirs = [local_db_dir / site_name for site_name in site_names]
    if localize_data:
        [out_dir.mkdir(exist_ok=True, parents=True) for out_dir in out_dirs]
    out_file_names = [f'site_name-{sn}-classified_planet-{pid}.tif' for sn, pid in zip(site_names, 
                                                                                       planet_ids)]

    def download_one_p(data):
        url, out_dir, out_file_name = data
        return download_one(url, out_dir, out_file_name=out_file_name) 
    input_data = list(zip(val_urls, out_dirs, out_file_names))
    n = len(input_data)
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        val_paths = list(tqdm(executor.map(download_one_p, input_data), total=n, desc='dwsx_files'))     
    return val_paths


# %%
dswx_paths_all = localize_dswx_data(df)

# %%
val_paths = localize_val_data(df)


# %% [markdown]
# ## Serialize Metadata from Planet Classification
#
# This includes all the notes from the manual/semi-automated classification.

# %%
def get_classification_metadata_and_notes(planet_id: str):
    metadata = df_planet[df_planet.image_name == planet_id].to_dict('records')[0]
    return metadata

def serialize_metadata_for_classified_dataset(data: dict):
    site_name = data['site_name']
    planet_id = data['planet_id']
    out_dir = local_db_dir / site_name
    out_path = out_dir / f'Site-{site_name}-metadata.json'
    metadata = get_classification_metadata_and_notes(planet_id)
    # Shapely geometries need to be converted to strings
    metadata['geometry'] = metadata['geometry'].wkt
    json.dump(metadata, open(out_path, 'w'), indent=2)
    return out_path


# %%
records = df.to_dict('records')
if LOCALIZE_DATA:
    metadata_paths = list(map(serialize_metadata_for_classified_dataset, records))

# %% [markdown]
# ## Update Table
#
# We are going to have the relative path to the `local_db_dir`.

# %%
N = len(dswx_paths_all) // 10
dswx_paths_all_relative = list(map(lambda p: p.relative_to(local_db_dir), dswx_paths_all))
dswx_paths_all_relative_str = list(map(str, dswx_paths_all_relative))
dswx_paths_grouped = [' '.join(dswx_paths_all_relative_str[10 * n: 10 * (n+1)]) for n in range(N)]
dswx_paths_grouped[0]

# %%
val_paths_relative = list(map(lambda p: p.relative_to(local_db_dir), val_paths))

# %%
df['rel_local_val_path'] = list(map(str, val_paths_relative))
df['rel_local_dswx_hls_paths'] = dswx_paths_grouped
df.head()

# %% [markdown]
# Save the metadata table inside the local database too.

# %%
df.to_file(local_db_dir / 'validation_table.geojson', driver='GeoJSON')

# %% [markdown]
# We are going to save the `dswx_version` for provenance of the generated data.

# %%
with open(local_db_dir / 'software_version.txt', 'w') as f:
    version=dswx_verification.__version__
    f.write(f'dswx_verification version: {version}')

# %%
if LOCALIZE_DATA:
    shutil.make_archive(local_db_dir, 'zip', local_db_dir)

# %% [markdown]
# # **WIP** DSWx-S1

# %%
import boto3
s3client = boto3.client('s3')
DSWx_S1_BUCKET_NAME = 'dswx-s1-adt-products'

paginator = s3client.get_paginator('list_objects')

# Specify additional parameters as needed
page_iterator = paginator.paginate(Bucket=DSWx_S1_BUCKET_NAME)
objs = []
for pg in page_iterator:
    if 'Contents' in pg:
        objs += pg['Contents']
    
# resp = s3client.list_objects(Bucket=DSWx_S1_BUCKET_NAME)
# objs = resp['Contents']
len(objs)


# %%
wtr_objs = [o for o in objs if o['Key'][-8:] == '_WTR.tif']
wtr_objs[:3], len(wtr_objs)

# %% [markdown]
# There are some duplicates

# %%
[wtr_obj for wtr_obj in wtr_objs if 'T47ULQ' in wtr_obj['Key']]


# %% [markdown]
# ## Get WTR urls

# %%
def extract_url(wtr_obj: dict) -> str:
    key = wtr_obj['Key']
    return f'https://{DSWx_S1_BUCKET_NAME}.s3.us-west-2.amazonaws.com/{key}'

wtr_urls = list(map(extract_url, wtr_objs))
wtr_urls[:2]

# %%
len(wtr_urls)

# %% [markdown]
# ## Get Extent Geometry 

# %%
from shapely.geometry import box, Polygon
from rasterio.crs import CRS

def get_extent_geo(url: str) -> tuple[Polygon, CRS]:
    with rasterio.open(url) as ds:
        extent = list(ds.bounds)
        crs = ds.crs
    return box(*extent), crs
#extent_geos = list(map(get_extent_geo, tqdm(wtr_urls)))
with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
    out = list(tqdm(executor.map(get_extent_geo, wtr_urls[:]), total=len(wtr_urls)))

# %%
extent_geos, crs_lst = zip(*out)

# %%
crs_lst[0]

# %%
extent_geos[0]

# %% [markdown]
# ## Reproject

# %%
import pyproj
from shapely.ops import transform
from itertools import starmap

crs_4326 = CRS.from_epsg(4326)

def reproject_one_geo_to_4326(geo: Polygon, utm_crs: CRS) -> Polygon:
    project = pyproj.Transformer.from_crs(utm_crs, crs_4326, always_xy=True).transform
    return transform(project, geo)


# %%
extent_geos_4326 = list(starmap(reproject_one_geo_to_4326, zip(extent_geos, tqdm(crs_lst))))

# %%
extent_geos_4326[0]


# %% [markdown]
# ## Get all other URLs

# %%
def get_prefix(wtr_obj: dict) -> str:
    key = wtr_obj['Key']

    fn = key.split('/')[-1]
    init_prefix = key[:-len(fn)]
    
    tokens = fn.split('_')
    prefix = init_prefix + '_'.join(tokens[:6])
    return prefix

def get_all_urls(wtr_obj: dict) -> list:
    prefix = get_prefix(wtr_obj)
    resp = s3client.list_objects(Bucket=DSWx_S1_BUCKET_NAME, Prefix=prefix)
    objs = resp['Contents']
    objs_tifs = [o for o in objs if o['Key'][-4:] == '.tif']
    return list(map(extract_url, objs_tifs))

def get_urls_str(wtr_obj: dict) -> str:
    urls_str = ' '.join(get_all_urls(wtr_obj))
    return urls_str


urls_str_lst = list(map(get_urls_str, tqdm(wtr_objs)))
urls_str_lst[-1]


# %% [markdown]
# ## Other Metadata

# %%
def get_tile_id(wtr_obj: dict) -> str:
    key = wtr_obj['Key']
    fn = key.split('/')[-1]
    return fn.split('_')[3]

def get_dswx_s1_id(wtr_obj: dict) -> str:
    key = wtr_obj['Key']
    fn = key.split('/')[-1].replace('_B01_WTR', '')
    return fn[:-4]


# %%
get_dswx_s1_id(wtr_objs[0])

# %%
mgrs_ids = list(map(get_tile_id, wtr_objs))
dswx_s1_ids = list(map(get_dswx_s1_id, wtr_objs))

# %% [markdown]
# ## Get DSWx-S1 Dataframe

# %%
df_dswx_s1 = gpd.GeoDataFrame({'dswx_s1_id': dswx_s1_ids,
                               'mgrs_tile_id': mgrs_ids,
                               'dswx_s1_urls': urls_str_lst},
                              geometry=extent_geos_4326,
                              crs=crs_4326)
df_dswx_s1 = df_dswx_s1.drop_duplicates(subset=['dswx_s1_id'])
df_dswx_s1.shape

# %%
df_dswx_s1.to_file('dswx_s1_data.geojson', driver='GeoJSON')

# %%
df_dswx_s1.plot()

# %%
df_dswx_s1.shape

# %% [markdown]
# ## Prepare for Spatial Join

# %%
cols_dswx_s1 = [col for col in df_dswx_s1.columns if col != 'geometry']
df_for_spatial_join = df[[col for col in df.columns if col not in cols_dswx_s1 + ['rel_local_dswx_paths']]].reset_index(drop=True)
print(df_for_spatial_join.shape)
df_for_spatial_join.drop_duplicates(subset=['site_name'], keep='first', inplace=True)
print(df_for_spatial_join.shape)


# %% [markdown]
# ##  Spatial join on Val Table

# %%
print(df_dswx_s1.shape)
df_dswx_s1_no_dups = df_dswx_s1.sort_values(by='dswx_s1_id').drop_duplicates(subset='mgrs_tile_id', keep='last').reset_index(drop=True)
df_dswx_s1_no_dups.shape

# %%
df_joined = gpd.sjoin(df_for_spatial_join, df_dswx_s1_no_dups, how='left', predicate='intersects')
df_joined.drop(columns=['index_right'], inplace=True)
print(df_joined.shape)
df_joined.head()

# %%
print(df_joined.dswx_s1_urls.isna().sum())
df_joined[df_joined.dswx_s1_urls.isna()].site_name.tolist()

# %%
df_all = df_joined.copy()

# %%
# duplicated_sites = df_all[df_all.duplicated(subset=['site_name'])].site_name.tolist()
# df_all[df_all.site_name.isin(duplicate_sites)].to_dict('records')[2:4]

# %% [markdown]
# # Serialize the Validation Table in package data
#
# This allows us to use the table in the actual package.

# %%
geojson_path = get_path_of_validation_geojson()

# %%
df_all.to_file(geojson_path, driver='GeoJSON')

# %%
df_all.to_csv(geojson_path.with_suffix('.csv'), index=False)
